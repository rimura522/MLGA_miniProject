{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b69150be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cabb3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'D:/rahul1/6th_sem/meta/output.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "951bf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89de0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data.to_csv('trainn.csv', index=False)\n",
    "test_data.to_csv('testt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79f39dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41419, 3)\n",
      "(10355, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)  \n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcde673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df=pd.read_csv('trainn.csv')\n",
    "test_df=pd.read_csv('testt.csv')\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "train_df[\"terms\"] = train_df[\"terms\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")\n",
    "from ast import literal_eval\n",
    "\n",
    "test_df[\"terms\"] = test_df[\"terms\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")\n",
    "from ast import literal_eval\n",
    "\n",
    "data[\"terms\"] = data[\"terms\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27ddc1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat.ML\n",
      "cs.AI\n",
      "cs.LG\n",
      "cs.CV\n",
      "cs.CL\n",
      "cs.HC\n",
      "cs.CR\n",
      "cs.CY\n",
      "cs.RO\n",
      "cs.IR\n",
      "eess.IV\n",
      "eess.SP\n",
      "cs.NE\n",
      "cs.SD\n",
      "eess.AS\n",
      "cs.LO\n",
      "cs.MM\n",
      "cs.AR\n",
      "math.OC\n",
      "cs.SY\n",
      "eess.SY\n",
      "cs.GR\n",
      "cs.DL\n",
      "cs.NA\n",
      "math.NA\n",
      "stat.AP\n",
      "stat.ME\n",
      "math.DS\n",
      "cs.DB\n",
      "cs.SI\n",
      "cs.CE\n",
      "cs.GT\n",
      "math.ST\n",
      "stat.TH\n",
      "cs.CG\n",
      "math.CA\n",
      "cs.SE\n",
      "cs.DC\n",
      "physics.comp-ph\n",
      "physics.data-an\n",
      "cs.FL\n",
      "cs.DS\n",
      "cs.MA\n",
      "cs.IT\n",
      "math.IT\n",
      "cs.PF\n",
      "physics.optics\n",
      "math.PR\n",
      "physics.bio-ph\n",
      "math.FA\n",
      "physics.class-ph\n",
      "stat.CO\n",
      "physics.soc-ph\n",
      "cs.DM\n",
      "math.DG\n",
      "physics.app-ph\n",
      "stat.OT\n",
      "physics.med-ph\n",
      "I.2.2\n",
      "05C60\n",
      "cs.PL\n",
      "cs.MS\n",
      "physics.chem-ph\n",
      "cs.CC\n",
      "physics.geo-ph\n",
      "physics.flu-dyn\n",
      "physics.ao-ph\n",
      "math.AP\n",
      "cs.NI\n",
      "cs.SC\n",
      "H.3.3\n",
      "math.CO\n",
      "math.GR\n",
      "math.RT\n",
      "cond-mat.stat-mech\n",
      "G.3\n",
      "math.AT\n",
      "physics.ins-det\n",
      "H.0\n",
      "math.OA\n",
      "math-ph\n",
      "math.MP\n",
      "physics.ed-ph\n",
      "physics.pop-ph\n",
      "math.MG\n",
      "cs.ET\n",
      "math.CT\n",
      "physics.acc-ph\n",
      "math.SP\n",
      "physics.plasm-ph\n",
      "68W50\n",
      "math.LO\n",
      "math.AC\n",
      "math.AG\n",
      "math.RA\n",
      "cs.OS\n",
      "C.1.3\n",
      "math.NT\n"
     ]
    }
   ],
   "source": [
    "unique_categories=[]\n",
    "for i in range(len(train_df.terms)):\n",
    "    for j in range(len(train_df.terms[i])):\n",
    "        if train_df.terms[i][j] not in unique_categories:\n",
    "            unique_categories.append(train_df.terms[i][j])\n",
    "            print(train_df.terms[i][j])\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "386cd589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['05C60', '68W50', 'C.1.3', 'G.3', 'H.0', 'H.3.3', 'I.2.2', 'I.5',\n",
       "       'cond-mat.stat-mech', 'cs.AI', 'cs.AR', 'cs.CC', 'cs.CE', 'cs.CG',\n",
       "       'cs.CL', 'cs.CR', 'cs.CV', 'cs.CY', 'cs.DB', 'cs.DC', 'cs.DL',\n",
       "       'cs.DM', 'cs.DS', 'cs.ET', 'cs.FL', 'cs.GR', 'cs.GT', 'cs.HC',\n",
       "       'cs.IR', 'cs.IT', 'cs.LG', 'cs.LO', 'cs.MA', 'cs.MM', 'cs.MS',\n",
       "       'cs.NA', 'cs.NE', 'cs.NI', 'cs.OS', 'cs.PF', 'cs.PL', 'cs.RO',\n",
       "       'cs.SC', 'cs.SD', 'cs.SE', 'cs.SI', 'cs.SY', 'eess.AS', 'eess.IV',\n",
       "       'eess.SP', 'eess.SY', 'math-ph', 'math.AC', 'math.AG', 'math.AP',\n",
       "       'math.AT', 'math.CA', 'math.CO', 'math.CT', 'math.CV', 'math.DG',\n",
       "       'math.DS', 'math.FA', 'math.GR', 'math.GT', 'math.HO', 'math.IT',\n",
       "       'math.LO', 'math.MG', 'math.MP', 'math.NA', 'math.NT', 'math.OA',\n",
       "       'math.OC', 'math.PR', 'math.RA', 'math.RT', 'math.SP', 'math.ST',\n",
       "       'physics.acc-ph', 'physics.ao-ph', 'physics.app-ph',\n",
       "       'physics.bio-ph', 'physics.chem-ph', 'physics.class-ph',\n",
       "       'physics.comp-ph', 'physics.data-an', 'physics.ed-ph',\n",
       "       'physics.flu-dyn', 'physics.geo-ph', 'physics.ins-det',\n",
       "       'physics.med-ph', 'physics.optics', 'physics.plasm-ph',\n",
       "       'physics.pop-ph', 'physics.soc-ph', 'stat.AP', 'stat.CO',\n",
       "       'stat.ME', 'stat.ML', 'stat.OT', 'stat.TH'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform(data.terms)\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fc878bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarized = []\n",
    "for i in range(len(train_df.terms)):\n",
    "#     print(i)\n",
    "    label_binarized.append(mlb.transform([train_df[\"terms\"].iloc[i]]))\n",
    "    \n",
    "sss=[]\n",
    "for i in label_binarized:\n",
    "    sss.append(i[0])\n",
    "\n",
    "train_df['binarized'] = sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbcf4fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarized = []\n",
    "for i in range(len(test_df.terms)):\n",
    "#     print(i)\n",
    "    label_binarized.append(mlb.transform([test_df[\"terms\"].iloc[i]]))\n",
    "    \n",
    "sss=[]\n",
    "for i in label_binarized:\n",
    "    sss.append(i[0])\n",
    "\n",
    "test_df['binarized'] = sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "134e8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp = train_df.drop(['terms'], axis=1)\n",
    "X_test_temp = test_df.drop(['terms'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d95cb98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_temp = train_df.terms\n",
    "y_test_temp = test_df.terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "828a7d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>binarized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Optimizing the CVaR via Sampling</td>\n",
       "      <td>Conditional Value at Risk (CVaR) is a prominen...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Graph Representation Learning for Vide...</td>\n",
       "      <td>Given an input video, its associated audio, an...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C2MSNet: A Novel approach for single image haz...</td>\n",
       "      <td>Degradation of image quality due to the presen...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HYPE: A Benchmark for Human eYe Perceptual Eva...</td>\n",
       "      <td>Generative models often use human evaluations ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cross-GCN: Enhancing Graph Convolutional Netwo...</td>\n",
       "      <td>Graph Convolutional Network (GCN) is an emergi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41414</th>\n",
       "      <td>3D Object Detection with Pointformer</td>\n",
       "      <td>Feature learning for 3D object detection from ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41415</th>\n",
       "      <td>Positional Contrastive Learning for Volumetric...</td>\n",
       "      <td>The success of deep learning heavily depends o...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41416</th>\n",
       "      <td>Passive attention in artificial neural network...</td>\n",
       "      <td>Developments in machine learning interpretabil...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41417</th>\n",
       "      <td>Task Driven Generative Modeling for Unsupervis...</td>\n",
       "      <td>Automatic parsing of anatomical objects in X-r...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41418</th>\n",
       "      <td>Subsampling Generative Adversarial Networks: D...</td>\n",
       "      <td>Filtering out unrealistic images from trained ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41419 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titles  \\\n",
       "0                       Optimizing the CVaR via Sampling   \n",
       "1      Dynamic Graph Representation Learning for Vide...   \n",
       "2      C2MSNet: A Novel approach for single image haz...   \n",
       "3      HYPE: A Benchmark for Human eYe Perceptual Eva...   \n",
       "4      Cross-GCN: Enhancing Graph Convolutional Netwo...   \n",
       "...                                                  ...   \n",
       "41414               3D Object Detection with Pointformer   \n",
       "41415  Positional Contrastive Learning for Volumetric...   \n",
       "41416  Passive attention in artificial neural network...   \n",
       "41417  Task Driven Generative Modeling for Unsupervis...   \n",
       "41418  Subsampling Generative Adversarial Networks: D...   \n",
       "\n",
       "                                               summaries  \\\n",
       "0      Conditional Value at Risk (CVaR) is a prominen...   \n",
       "1      Given an input video, its associated audio, an...   \n",
       "2      Degradation of image quality due to the presen...   \n",
       "3      Generative models often use human evaluations ...   \n",
       "4      Graph Convolutional Network (GCN) is an emergi...   \n",
       "...                                                  ...   \n",
       "41414  Feature learning for 3D object detection from ...   \n",
       "41415  The success of deep learning heavily depends o...   \n",
       "41416  Developments in machine learning interpretabil...   \n",
       "41417  Automatic parsing of anatomical objects in X-r...   \n",
       "41418  Filtering out unrealistic images from trained ...   \n",
       "\n",
       "                                               binarized  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "41414  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "41415  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "41416  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "41417  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "41418  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[41419 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c86d84fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65c33d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label shape torch.Size([41419, 102])\n",
      "Test label shape torch.Size([10355, 102])\n"
     ]
    }
   ],
   "source": [
    "text_train = X_train_temp.summaries.values\n",
    "text_test = X_test_temp.summaries.values\n",
    "\n",
    "y_train = torch.from_numpy(np.vstack(X_train_temp.binarized.values)).float()\n",
    "y_test = torch.from_numpy(np.vstack(X_test_temp.binarized.values)).float()\n",
    "\n",
    "print('Train label shape {}'.format(y_train.shape))\n",
    "print('Test label shape {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52397864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\rahul\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (21.3)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: namex in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: rich in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: optree in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.16.1->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62f807d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp.binarized\n",
    "df_train=pd.DataFrame({'summaries':text_train,'terms':y_train_temp,'binarized':X_train_temp.binarized.values})\n",
    "df_test=pd.DataFrame({'summaries':text_test,'terms':y_test_temp,'binarized':X_test_temp.binarized.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5149551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size): \n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = F.relu(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded\n",
    "    \n",
    "class GraphAttentionLayer(nn.Module):\n",
    "  def __init__(self, inp, out, slope):\n",
    "    super(GraphAttentionLayer, self).__init__()\n",
    "    self.W = nn.Linear(inp, out, bias=False)\n",
    "    self.a = nn.Linear(out*2, 1, bias=False)\n",
    "    self.leakyrelu = nn.LeakyReLU(slope)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, h, adj):\n",
    "    Wh = self.W(h)\n",
    "    Whcat = self.Wh_concat(Wh, adj)\n",
    "    e = self.leakyrelu(self.a(Whcat).squeeze(2))\n",
    "    zero_vec = -9e15*torch.ones_like(e)\n",
    "    attention = torch.where(adj > 0, e, zero_vec)\n",
    "    attention = self.softmax(attention)\n",
    "    h_hat = torch.mm(attention, Wh)\n",
    "\n",
    "    return h_hat\n",
    " \n",
    "  def Wh_concat(self, Wh, adj):\n",
    "    N = Wh.size(0)\n",
    "    Whi = Wh.repeat_interleave(N, dim=0)\n",
    "    Whj = Wh.repeat(N, 1)\n",
    "    WhiWhj = torch.cat([Whi, Whj], dim=1)\n",
    "    WhiWhj = WhiWhj.view(N, N, Wh.size(1)*2)\n",
    "\n",
    "    return WhiWhj\n",
    " \n",
    "class MultiHeadGAT(nn.Module):\n",
    "  def __init__(self, inp, out, heads, slope):\n",
    "    super(MultiHeadGAT, self).__init__()\n",
    "    self.attentions = nn.ModuleList([GraphAttentionLayer(inp, out, slope) for _ in range(heads)])\n",
    "    self.tanh = nn.Tanh()\n",
    "  \n",
    "  def forward(self, h, adj):\n",
    "    heads_out = [att(h, adj) for att in self.attentions]\n",
    "    out = torch.stack(heads_out, dim=0).mean(0)\n",
    "    \n",
    "    return self.tanh(out)\n",
    " \n",
    "class GAT(nn.Module):\n",
    "  def __init__(self, inp, out, heads, slope=0.01):\n",
    "    super(GAT, self).__init__()\n",
    "    self.gat1 = MultiHeadGAT(inp, out, heads, slope)\n",
    "    self.gat2 = MultiHeadGAT(out, out, heads, slope)\n",
    "  \n",
    "  def forward(self, h, adj):\n",
    "    out = self.gat1(h, adj)\n",
    "    out = self.gat2(out, adj)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Using static embedding\n",
    "class MAGNET(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, adjacency, embeddings, heads=3, slope=0.05, dropout=0.4):\n",
    "        super(MAGNET, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.rnn = nn.LSTM(input_size,\n",
    "                           hidden_size,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=True)\n",
    "        self.gat = GAT(input_size, hidden_size*2, heads, slope)\n",
    "        \n",
    "        self.adjacency = nn.Parameter(adjacency)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "#         self.predicted_values = None\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, token, label_embedding):\n",
    "        features = self.embedding(token)\n",
    "        \n",
    "        out, (hidden, cell) = self.rnn(features)\n",
    "        \n",
    "        out = torch.cat([hidden[-2, :, :], hidden[-1, :, :]], dim=1)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        att = self.dropout(self.gat(label_embedding, self.adjacency))\n",
    "        att = att.transpose(0, 1)\n",
    "        \n",
    "        out = torch.mm(out, att)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, token, label_embedding):\n",
    "        # run the input data through the model\n",
    "        output = self.forward(token, label_embedding)\n",
    "        # apply a sigmoid function to the output to get the probabilities\n",
    "        probabilities = torch.sigmoid(output)\n",
    "        # round the probabilities to get the final predictions\n",
    "        predictions = probabilities.round()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7a45c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contextual Embedding\n",
    "\n",
    "class ContextMAGNET(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, adjacency, heads=3, slope=0.01, dropout=0.3):\n",
    "    super(ContextMAGNET, self).__init__()\n",
    "\n",
    "    self.rnn = nn.LSTM(input_size,\n",
    "                        hidden_size,\n",
    "                        batch_first=True,\n",
    "                        bidirectional=True)\n",
    "\n",
    "    self.gat = GAT(input_size, hidden_size*2, heads, slope)\n",
    "    \n",
    "    self.adjacency = nn.Parameter(adjacency)\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "  def forward(self, features, label_embedding):\n",
    "\n",
    "    out, (hidden, cell) = self.rnn(features)\n",
    "    \n",
    "    out = torch.cat([hidden[-2, :, :], hidden[-1, :, :]], dim=1)\n",
    "    out = self.dropout(out)\n",
    "    \n",
    "    att = self.dropout(self.gat(label_embedding, self.adjacency))\n",
    "    att = att.transpose(0, 1)\n",
    "    \n",
    "    out = torch.mm(out, att)\n",
    " \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52b48804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build adjacency matrix based on Co-Occurencies label\n",
    "def buildAdjacencyCOOC(data_label):\n",
    "  adj = data_label.T.dot(data_label).astype('float')\n",
    "  for i in range(len(adj)):\n",
    "    adj[i] = adj[i] / adj[i,i]\n",
    "  \n",
    "  return torch.from_numpy(adj.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6da2d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.init import kaiming_normal_, xavier_normal_\n",
    "\n",
    "def buildAdjacencyCOOC(data_label):\n",
    "    # Transpose the data_label matrix\n",
    "    adj = data_label.T\n",
    "    \n",
    "    # Calculate the dot product of the transposed matrix and the original matrix\n",
    "    adj = adj.dot(data_label)\n",
    "    \n",
    "    # Convert the adjacency matrix to a PyTorch tensor\n",
    "    adj = torch.from_numpy(adj.astype('float32'))\n",
    "    \n",
    "    # Initialize the weights of the adjacency matrix using Kaiming (He) initialization\n",
    "    kaiming_normal_(adj)\n",
    "    #   xavier_normal_(adj)\n",
    "    \n",
    "    # Normalize the matrix by dividing each row by its diagonal element\n",
    "    for i in range(len(adj)):\n",
    "        adj[i] = adj[i] / adj[i,i]\n",
    "        \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6c8b3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  5.7484, -1.8380,  ...,  0.9218, -4.1191,  0.2744],\n",
      "        [-2.9148,  1.0000, -4.7635,  ...,  2.7126, -2.9875,  4.7088],\n",
      "        [-1.9639, -1.0364,  1.0000,  ...,  2.2977,  0.2156,  2.9549],\n",
      "        ...,\n",
      "        [-0.4617,  0.3184, -2.9385,  ...,  1.0000, -1.0176,  4.1818],\n",
      "        [ 0.0087, -0.1472, -1.1719,  ...,  0.0265,  1.0000, -0.2737],\n",
      "        [ 3.4640, -0.5930, -0.5176,  ...,  2.9801,  1.0865,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "adjacency = buildAdjacencyCOOC(y_train.numpy())\n",
    "print(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "52d0a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Text cleaning function\n",
    "def preprocessingText(text, stop=stop):\n",
    "  text = text.lower() #text to lowercase\n",
    "  text = re.sub(r'&lt;', '', text) #remove '&lt;' tag\n",
    "  text = re.sub(r'<.*?>', '', text) #remove html\n",
    "  text = re.sub(r'[0-9]+', '', text) #remove number\n",
    "  text = \" \".join([word for word in text.split() if word not in stop]) #remove stopwords\n",
    "  text = re.sub(r'[^\\w\\s]', '', text) #remove punctiation\n",
    "  text = re.sub(r'[^\\x00-\\x7f]', '', text) #remove non ASCII strings\n",
    "  for c in ['\\r', '\\n', '\\t'] :\n",
    "    text = re.sub(c, ' ', text) #replace newline and tab with tabs\n",
    "  text = re.sub('\\s+', ' ', text) #replace multiple spaces with one space\n",
    "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "  return text\n",
    "\n",
    "#Load Word Representation Vector\n",
    "def loadWRVModel(File):\n",
    "    print(\"Loading Word Representation Vector Model\")\n",
    "    f = open(File,'r')\n",
    "    WRVModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        try:\n",
    "          wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        except:\n",
    "          print(splitLines[1:])\n",
    "          print(len(splitLines[1:]))\n",
    "          break\n",
    "        WRVModel[word] = wordEmbedding\n",
    "    print(len(WRVModel),\" words loaded!\")\n",
    "    return WRVModel\n",
    "\n",
    "\n",
    "def check_accuracy(model, label_embedding, X, y):\n",
    "  \n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    out = model(X, label_embedding)\n",
    "    y_pred = torch.sigmoid(out.detach()).round().cpu()\n",
    "    f1score = f1_score(y, y_pred, average='micro')\n",
    "    hammingloss = hamming_loss(y, y_pred)\n",
    "  \n",
    "  return hammingloss, f1score\n",
    "\n",
    "def train(model,\n",
    "          X_train,\n",
    "          X_test,\n",
    "          label_embedding,\n",
    "          y_train,\n",
    "          y_test,\n",
    "          total_epoch=50,\n",
    "          batch_size=250,\n",
    "          learning_rate=0.001,\n",
    "          save_path='./model.pt',\n",
    "          state=None):\n",
    "  \n",
    "  device = torch.device(\"cpu\")#\"cuda\" if torch.cuda.is_available() else \n",
    "\n",
    "  train_data = DataLoader(dataset(X_train, y_train), batch_size=batch_size)\n",
    "  X_test = X_test.to(device)\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  label_embedding= label_embedding.to(device)\n",
    "\n",
    "  if state:\n",
    "    state = torch.load(state)\n",
    "    model = model.load_state_dict(state['last_model'])\n",
    "    optimizer = optimizer.load_state_dict(state['optimizer'])\n",
    "  \n",
    "  else:\n",
    "    model = model.to(device)\n",
    "    state = dict()\n",
    "    state['microf1'] = []\n",
    "    state['hammingloss'] = []\n",
    "    state['val_hammingloss'] = []\n",
    "    state['val_microf1'] = []\n",
    "    state['epoch_time'] = []\n",
    "    \n",
    "  epoch = 1\n",
    "  \n",
    "  best_train = 0\n",
    "  best_val = 0\n",
    "  \n",
    "  while epoch <= total_epoch:\n",
    "    running_loss = 0\n",
    "    y_pred = []\n",
    "    epoch_time = 0\n",
    "    model.train()\n",
    "    for index, (X, y) in enumerate(train_data):\n",
    "      \n",
    "      t = time.time()\n",
    "\n",
    "      #forward\n",
    "      out = model(X.to(device), label_embedding)\n",
    "      loss = criterion(out, y.to(device))\n",
    "\n",
    "      #backward\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "      #update\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_time += time.time() - t\n",
    "      y_pred.append(torch.sigmoid(out.detach()).round().cpu())\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    y_pred = torch.vstack(y_pred)\n",
    "    f1score = f1_score(y_train, y_pred, average='micro')\n",
    "    hammingloss = hamming_loss(y_train, y_pred)\n",
    "    val_hamming, val_f1score = check_accuracy(model, label_embedding, X_test, y_test)\n",
    "\n",
    "    state['microf1'].append(f1score)\n",
    "    state['hammingloss'].append(hammingloss)\n",
    "    state['val_microf1'].append(val_f1score)\n",
    "    state['epoch_time'].append(epoch_time)\n",
    "    state['val_hammingloss'].append(val_hamming)\n",
    "\n",
    "    state['optimizer'] = optimizer.state_dict()\n",
    "    state['last_model'] = model.state_dict()\n",
    "    \n",
    "\n",
    "    \n",
    "    if(best_train < f1score):\n",
    "      state['model_best_train'] = copy.deepcopy(model.state_dict())\n",
    "      best_train = f1score\n",
    "      state['best_train'] = best_train\n",
    "    \n",
    "    if(best_val < val_f1score):\n",
    "      state['model_best_val'] = copy.deepcopy(model.state_dict())\n",
    "      best_val = val_f1score\n",
    "      state['best_val'] = best_val\n",
    "\n",
    "    torch.save(state, save_path)\n",
    "    print('epoch:{} loss:{:.5f} hamming_loss:{:.5f} micro_f1score:{:.5f} val_hamming_loss:{:.5f} val_micro_f1score:{:.5f}'.\n",
    "          format(epoch, running_loss, hammingloss, f1score, val_hamming, val_f1score))\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4dd65eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word Representation Vector Model\n",
      "0  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# # import sys\n",
    "# # csv.field_size_limit(2 * 10**9)\n",
    "# # maxInt = sys.maxsize\n",
    "\n",
    "# # while True:\n",
    "# #     # decrease the maxInt value by factor 10 \n",
    "# #     # as long as the OverflowError occurs.\n",
    "\n",
    "# #     try:\n",
    "# #         csv.field_size_limit(maxInt)\n",
    "# #         break\n",
    "# #     except OverflowError:\n",
    "# #         maxInt = int(maxInt/10)\n",
    "# new = open('D:/rahul1/6th_sem/meta/glove.6B.300d.txt', encoding=\"utf8\")\n",
    "# with open('D:/rahul1/6th_sem/meta/shortver.txt', \"r\", encoding=\"utf8\") as text_file:\n",
    "#     text_reader = csv.reader(text_file, delimiter='|')\n",
    "#     with open('D:/rahul1/6th_sem/meta/w2v.csv', 'w') as csv_file:\n",
    "#         csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "#         csv_writer.writerows(text_reader)\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('D:/rahul1/6th_sem/meta/glove.6B.300d.txt', sep='|', encoding='utf-8', iterator=True, chunksize=10000)  # Adjust chunksize as needed\n",
    "# for chunk in df:\n",
    "#     chunk.to_csv('D:/rahul1/6th_sem/meta/w2v.csv', header=False, index=False, mode='a') \n",
    "WRVModel = loadWRVModel('D:/rahul1/6th_sem/meta/shorterver.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc2f5cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CLEANING: Conditional Value at Risk (CVaR) is a prominent risk measure that is being\n",
      "used extensively in various domains. We develop a new formula for the gradient\n",
      "of the CVaR in the form of a conditional expectation. Based on this formula, we\n",
      "propose a novel sampling-based estimator for the CVaR gradient, in the spirit\n",
      "of the likelihood-ratio method. We analyze the bias of the estimator, and prove\n",
      "the convergence of a corresponding stochastic gradient descent algorithm to a\n",
      "local CVaR optimum. Our method allows to consider CVaR optimization in new\n",
      "domains. As an example, we consider a reinforcement learning application, and\n",
      "learn a risk-sensitive controller for the game of Tetris.\n",
      "AFTER CLEANING: conditional value risk cvar prominent risk measure used extensively various domain develop new formula gradient cvar form conditional expectation based formula propose novel samplingbased estimator cvar gradient spirit likelihoodratio method analyze bias estimator prove convergence corresponding stochastic gradient descent algorithm local cvar optimum method allows consider cvar optimization new domain example consider reinforcement learning application learn risksensitive controller game tetri\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "preprocessed_text_train = [preprocessingText(text) for text in text_train]\n",
    "preprocessed_text_test = [preprocessingText(text) for text in text_test]\n",
    "\n",
    "print('BEFORE CLEANING: {}'.format(text_train[0]))\n",
    "print('AFTER CLEANING: {}'.format(preprocessed_text_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "68a62fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_text_train)\n",
    "sequences_text_train = tokenizer.texts_to_sequences(preprocessed_text_train)\n",
    "sequences_text_test = tokenizer.texts_to_sequences(preprocessed_text_test)\n",
    "\n",
    "X_train = torch.from_numpy(pad_sequences(sequences_text_train, maxlen=128))\n",
    "X_test = torch.from_numpy(pad_sequences(sequences_text_test, maxlen=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8d2d2b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE : 75998\n",
      "TOTAL OF UNKNOWN WORD : 75997\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = torch.zeros(VOCAB_SIZE, 300)\n",
    "\n",
    "unk = 0\n",
    "for i in range(1, VOCAB_SIZE):\n",
    "  word = tokenizer.index_word[i]\n",
    "  if word in WRVModel.keys():\n",
    "    embedding_matrix[i] = torch.from_numpy(WRVModel[word]).float()\n",
    "  else:\n",
    "    unk +=1\n",
    "print('VOCAB_SIZE : {}'.format(VOCAB_SIZE))\n",
    "print('TOTAL OF UNKNOWN WORD : {}'.format(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "90b9e920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "label_embedding = torch.zeros(102,300)\n",
    "\n",
    "for index, label in enumerate(mlb.classes_):\n",
    "  words = label.split('-')\n",
    "  num_of_words = len(words)\n",
    "\n",
    "  for sublabel in words:\n",
    "    if sublabel in WRVModel.keys():\n",
    "      label_embedding[index] +=  torch.from_numpy(WRVModel[sublabel])\n",
    "  label_embedding[index] = label_embedding[index]/num_of_words\n",
    "\n",
    "print(label_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c8c9481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.x  = x\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a8da2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAGNET(300, 150, adjacency, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b61a97e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1590528000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16960/3779160053.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16960/6987668.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, X_train, X_test, label_embedding, y_train, y_test, total_epoch, batch_size, learning_rate, save_path, state)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mf1score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mhammingloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhamming_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[0mval_hamming\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_f1score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'microf1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16960/6987668.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[1;34m(model, label_embedding, X, y)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mf1score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16960/1259416234.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, token, label_embedding)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    814\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1590528000 bytes."
     ]
    }
   ],
   "source": [
    "train(model, X_train, X_test, label_embedding, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6bc32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
